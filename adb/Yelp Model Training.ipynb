{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13c15d53-f8cf-4750-9921-1f004349c9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Download Training Data and Upload to Azure Data Lake Storage Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0544fa1-0515-4b6e-9243-891abf0dac03",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Please copy traning data from \n",
    "https://drive.google.com/file/d/1uNsbvMDz7Zz5cyskjNe0HL5LLXlpVvdX/view?usp=share_link to your ADLS container. \n",
    "\n",
    "This data is used for model training. \n",
    "\n",
    "In this notebook, we will use machine learning and apply NLP techniques to train a machine learning model. The model will use `Reviews` data to predict ratings \n",
    "\n",
    "What we are going to do:\n",
    "- Step 1: Prepare the training data for the machine learning training. \n",
    "- Step 2: Train the machine learning model;\n",
    "- Step 3: Save the model to a Azure storage folder so that you can use it for future prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialise SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ModelTraining').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28be8b6d-5a0b-4402-b5de-6e6a66c7adf6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Set Path to Training Data and Read to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b00001d4-6da2-43e2-a44e-51602cddd155",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set file path to the location of the downloaded training data in Azure datalake\n",
    "file_location = 'mnt/bd-Project/yelp-training-data/*'\n",
    "\n",
    "\n",
    "reviews = spark.read\\\n",
    "  .parquet(file_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd524cea-c9e1-4ba5-92c9-587c83b13688",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Print Training Data Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31cbd0a6-8f64-4e7a-b7e4-39896dcc8e74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|ORL4JE6tz3rJxVqkd...|   0|2015-03-22 19:01:49|    0|RdDRv8WuATj_19ltu...|  4.0|I remember stayin...|     0|FPOLMElOP7Xpqlwgo...|\n",
      "|RCy4M2ND4YK0uRbod...|   0|2015-04-02 16:52:19|    0|EMJDxWocRuQ-6HsVT...|  4.0|Convenient locati...|     2|vScaSrM91Z43ypSR9...|\n",
      "|VN2CJfXX6ooJt-Nc3...|   1|2014-11-18 15:31:43|    0|zLD4GdfIjaXZF-cUH...|  5.0|It's huge- you do...|     1|RPrbFB_bcot5TdNvj...|\n",
      "|GInRkBWvuyJCjFVHY...|   0|2018-03-24 00:18:33|    0|L4DxZ-PGArRpOdEGS...|  1.0|I'm not sure what...|     0|XVCAuOwGZHwtytJal...|\n",
      "|M4kHDHNzftSUtgpgy...|   0|2017-12-07 19:35:05|    0|OBpNmfGl2ysBpLJ1_...|  5.0|Finally, finally ...|     0|uFB375chEpTFuyP0d...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd52ce7e-c518-4942-9f0f-f1d598620da3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|stars|\n",
      "+--------------------+-----+\n",
      "|I remember stayin...|  4.0|\n",
      "|Convenient locati...|  4.0|\n",
      "|It's huge- you do...|  5.0|\n",
      "|I'm not sure what...|  1.0|\n",
      "|Finally, finally ...|  5.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = reviews.select(\"text\", \"stars\")\n",
    "reviews.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca573d9a-a78f-4b7d-a4a3-aab1b2b8bcd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "698757"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the dataframe to cache for repetitive use\n",
    "reviews.cache()\n",
    "reviews.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4052963e-ef43-4ad7-95c5-1a0026e371a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Prepare Data for Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e07514b5-b01c-40e4-991a-9c14fe4b7cb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, regexp_replace\n",
    "\n",
    "\n",
    "def prepare_data(df):\n",
    "    # Select only the \"text\" and \"stars\" columns from the DataFrame\n",
    "    df = df.select(\"text\", \"stars\")\n",
    "    \n",
    "    cleaned = (\n",
    "        # Convert to lowercase\n",
    "        df.withColumn(\"text\", lower(df.text))  \n",
    "\n",
    "        # Remove non-alphabetic characters\n",
    "        .withColumn(\"text\", regexp_replace(\"text\", \"[^a-zA-Z\\\\s]\", \"\")) \n",
    "\n",
    "        # Replace multiple spaces with one \n",
    "        .withColumn(\"text\", regexp_replace(\"text\", \"\\\\s+\", \" \"))  \n",
    "    )\n",
    "    \n",
    "    # Return the cleaned DataFrame with processed text\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b92a921-dba3-428b-938d-87d821414df7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creates & Train pipeline for text classification using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42cdafd-8635-4e54-9188-6516d0a98334",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.feature import IDF, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def create_and_train_pipeline(cleaned):\n",
    "    # Split the cleaned DataFrame into training and testing sets (80% train, 10% test)\n",
    "    train, test = cleaned.randomSplit([0.8, 0.1], seed=2024)\n",
    "    \n",
    "    # Initialize a tokenizer to convert the \"text\" column into individual tokens\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "    \n",
    "    # Initialize a stopword remover to filter out common stopwords from the tokens\n",
    "    stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "    \n",
    "    # Create a count vectorizer to convert the filtered tokens into a feature vector\n",
    "    cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "    \n",
    "    # Initialize an IDF transformer to adjust the feature vector based on document frequency\n",
    "    idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5)\n",
    "    \n",
    "    # Create a label encoder to convert the \"stars\" column into a numerical label\n",
    "    label_encoder = StringIndexer(inputCol=\"stars\", outputCol=\"label\")\n",
    "    \n",
    "    # Initialize a logistic regression model with a maximum of 100 iterations\n",
    "    lr = LogisticRegression(maxIter=100, regParam=0.1, elasticNetParam=0.0)\n",
    "    \n",
    "    # Create a pipeline that chains together all the stages defined above\n",
    "    pipeline = Pipeline(stages=[tokenizer, stopword_remover, cv, idf, label_encoder, lr])\n",
    "    \n",
    "    # Fit the pipeline model to the training data\n",
    "    pipeline_model = pipeline.fit(train)\n",
    "    \n",
    "    # Transform the test data using the fitted pipeline model to generate predictions\n",
    "    predictions = pipeline_model.transform(test)\n",
    "    \n",
    "    # Return the predictions and the fitted pipeline model\n",
    "    return predictions, pipeline_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aad9cd41-0b34-4a3d-b25f-28eb6447184d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Evaluates model's accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c243678-9d80-4aaa-b9d2-6839b102b327",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.6579255353353483\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Prepare the data by cleaning it and selecting relevant columns\n",
    "cleaned_data = prepare_data(reviews)\n",
    "\n",
    "# Create and train the machine learning pipeline, obtaining predictions and the fitted model\n",
    "predictions, pipeline_model = create_and_train_pipeline(cleaned_data)\n",
    "\n",
    "# Initialize an evaluator to assess the model's performance using accuracy as the metric\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Evaluate the predictions made by the model and compute the accuracy\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Print the accuracy of the model on the test set\n",
    "print(f\"Test set accuracy = {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71ec1995-c756-4b2d-a1ea-8cecf3612ff1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Save the Model file to Azure storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7079e709-854f-401c-a287-da93eaab574c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models successfully saved to specified locations\n"
     ]
    }
   ],
   "source": [
    "# Saves pipeline model object to  mnt/bd-Project/LRModel directory\n",
    "pipeline_model.save('mnt/bd-Project/LRModel')\n",
    "\n",
    "# Access the fitted label encoder model from the pipeline model since it is the second to last stage\n",
    "le_model = pipeline_model.stages[-2]\n",
    "\n",
    "# Saves label encoder model to  mnt/bd-Project/StringIndexer directory\n",
    "le_model.save('mnt/bd-Project/StringIndexer')\n",
    "\n",
    "print('models successfully saved to specified locations')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Yelp Model Training GitVersion",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
