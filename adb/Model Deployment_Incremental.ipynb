{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9373f17-d7f4-42ec-85ec-1caf3d8e0f4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import packages and create spark session\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"ML Model\")\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4fe9b11-a147-45f7-9931-86f80cd8633e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a UDF (User Defined Function) function to filter and transform \n",
    "# the data and generate model result\n",
    "\n",
    "# User defined function\n",
    "def transform_and_predict(df, ml_model, stringindexer):\n",
    "    from pyspark.sql.functions import col, regexp_replace, lower, trim\n",
    "    from pyspark.ml import PipelineModel\n",
    "\n",
    "    # Preprocessing of the feature column\n",
    "    df = df.withColumn(\"text\", lower(df.text)) # Lowercase Conversion\n",
    "\n",
    "    # Remove non letter characters and replace with nothing\n",
    "    df = df.withColumn(\"text\", regexp_replace(\"text\", \"[^a-zA-Z\\\\s]\", \"\")) \n",
    "\n",
    "    # Remove Extra Spaces\n",
    "    df = df.withColumn(\"text\", regexp_replace(\"text\", \"\\\\s+\", \" \")) \n",
    "\n",
    "    # Remove Leading and Trailing Spaces\n",
    "    df = df.withColumn(\"text\", trim(df.text))\n",
    "\n",
    "    # Load the saved machine learning pipeline model\n",
    "    model = PipelineModel.load(ml_model)\n",
    "\n",
    "    # Making the prediction\n",
    "    prediction = model.transform(df)\n",
    "\n",
    "    # predicted = prediction.select(col('text'), col('prediction'))\n",
    "   \n",
    "    # Decoding the indexer\n",
    "    from pyspark.ml.feature import StringIndexerModel, IndexToString\n",
    "\n",
    "    # Load in the StringIndexer that was saved\n",
    "    indexer = StringIndexerModel.load(stringindexer)\n",
    "\n",
    "    # Initialize the IndexToString converter\n",
    "    i2s = IndexToString(inputCol = 'prediction', outputCol = 'decoded', labels = indexer.labels)\n",
    "    # converted = i2s.transform(predicted)\n",
    "    converted = i2s.transform(prediction)\n",
    "\n",
    "    # Display the important columns\n",
    "    return converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be19ce06-cf37-4224-a3b5-b4ccdc9a1b30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define this function\n",
    "\n",
    "def saveFiles_multiple_call(ratings_path, ratings_agg_path, dfR, dfRAgg):\n",
    "\n",
    "    # write the result to a folder container several files\n",
    "  \n",
    "    dfR.coalesce(1).write.option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(ratings_path)\n",
    "\n",
    "    dfRAgg.coalesce(1).write.option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(ratings_agg_path)\n",
    "    \n",
    "    print('files successfully saved') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6ab616-e45b-4a6f-bc14-13b11ec0021e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dt=2024-05-05/', 'dt=2024-05-07/', 'dt=2024-05-22/', 'dt=2024-05-25/', 'dt=2024-05-26/', 'dt=2024-05-27/', 'dt=2024-05-30/', 'dt=2024-06-02/', 'dt=2024-06-03/', 'dt=2024-06-06/', 'dt=2024-06-08/', 'dt=2024-06-09/', 'dt=2024-06-10/', 'dt=2024-06-12/', 'dt=2024-06-13/', 'dt=2024-06-14/', 'dt=2024-06-16/', 'dt=2024-06-17/', 'dt=2024-06-18/', 'dt=2024-06-19/', 'dt=2024-06-20/', 'dt=2024-06-22/', 'dt=2024-06-24/', 'dt=2024-06-26/', 'dt=2024-06-27/', 'dt=2024-07-02/', 'dt=2024-07-03/', 'dt=2024-07-04/', 'dt=2024-07-06/', 'dt=2024-07-08/', 'dt=2024-07-09/', 'dt=2024-07-10/', 'dt=2024-07-12/']\n",
      "['dt=2024-05-05.csv', 'dt=2024-05-07.csv', 'dt=2024-05-22.csv', 'dt=2024-05-25.csv', 'dt=2024-05-26.csv', 'dt=2024-05-27.csv', 'dt=2024-05-30.csv', 'dt=2024-06-02.csv', 'dt=2024-06-03.csv', 'dt=2024-06-06.csv', 'dt=2024-06-08.csv', 'dt=2024-06-09.csv', 'dt=2024-06-10.csv', 'dt=2024-06-12.csv', 'dt=2024-06-13.csv', 'dt=2024-06-14.csv', 'dt=2024-06-16.csv', 'dt=2024-06-17.csv', 'dt=2024-06-18.csv', 'dt=2024-06-19.csv', 'dt=2024-06-20.csv', 'dt=2024-06-22.csv', 'dt=2024-06-24.csv', 'dt=2024-06-26.csv', 'dt=2024-06-27.csv', 'dt=2024-07-02.csv', 'dt=2024-07-03.csv', 'dt=2024-07-04.csv', 'dt=2024-07-06.csv', 'dt=2024-07-08.csv', 'dt=2024-07-09.csv']\n",
      "Folders name in source that is missing in final:\n",
      "dt=2024-07-10/\n",
      "dt=2024-07-12/\n"
     ]
    }
   ],
   "source": [
    "# Function to extract date from folder/file name\n",
    "def extract_date(name):\n",
    "    return name.split('=')[1]\n",
    "\n",
    "\n",
    "\n",
    "# Define the path to the daily files source\n",
    "src_path = \"/mnt/bd-project/de-yelp-daily/\"\n",
    "\n",
    "# Define the path to the final ratings data folder\n",
    "final_path = \"/mnt/bd-project/final-ratings/\"\n",
    "\n",
    "# get the list of folders in source\n",
    "src_list = dbutils.fs.ls(src_path)\n",
    "src_name_list = [item.name for item in src_list if item.isDir]\n",
    "print(src_name_list)\n",
    "\n",
    "# List files in final folder\n",
    "final_list = dbutils.fs.ls(final_path)\n",
    "final_name_list = [item.name for item in final_list if item.isFile()]\n",
    "print(final_name_list)\n",
    "\n",
    "# Extract dates from folder names in source folder\n",
    "dates_in_src = sorted([extract_date(folder_name) for folder_name in src_name_list])\n",
    "\n",
    "# Extract dates from file names in final\n",
    "temp = sorted([extract_date(file_name) for file_name in final_name_list])\n",
    "dates_in_final = [item.split(\".\")[0] + \"/\" for item in temp]\n",
    "\n",
    "\n",
    "# Find missing files\n",
    "missing_folders = \\\n",
    "    [folder_name for folder_name in dates_in_src if folder_name not in dates_in_final]\n",
    "\n",
    "# Print the list of folders in source folder whose equivalent file names is missing in final folder\n",
    "print(\"Folders name in source that is missing in final:\")\n",
    "\n",
    "if len(missing_folders) == 0:\n",
    "    print(\"No missing folders to process\")\n",
    "else:\n",
    "    for folder in missing_folders:\n",
    "        print(f\"dt={folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3792a0b-59da-423e-abb8-c08c31a00244",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# copy only csv files of ratings and aggregated ratings from temporary \n",
    "# folder to permanent folder\n",
    "def CopyCsv(source_dir, target_dir, item):\n",
    "\n",
    "    # List all files in the source directory\n",
    "    files = dbutils.fs.ls(source_dir)\n",
    "\n",
    "    # Filter for .csv files\n",
    "    csv_files = [item for item in files if item.name.endswith('.csv')]\n",
    "\n",
    "    # Since only one CSV file expected to be in folder\n",
    "    if len(csv_files) == 1:\n",
    "        source_file_path = csv_files[0].path\n",
    "        \n",
    "        target_file_path = target_dir + \"/\" + item + \".csv\"\n",
    "        dbutils.fs.cp(source_file_path, target_file_path)\n",
    "        print(\"copy operation successful\")\n",
    "    else:\n",
    "        print(\"Error: Expected 1 CSV file, found multiple.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "885e43f1-e051-4e31-abd7-cdde76be8299",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# delete contents of the temporary ratings and the temporary \n",
    "# aggregated ratings folders\n",
    "\n",
    "def RemoveFolderContents(path):\n",
    "    items = dbutils.fs.ls(path)\n",
    "\n",
    "    # Remove each item\n",
    "    for item in items:\n",
    "        dbutils.fs.rm(item.path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55aa08eb-b902-469b-af47-de13c8d69ad7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10/\n",
      "dt=2024-07-10\n",
      "temp ratings path: /mnt/bd-project/temp-ratings-result/dt=2024-07-10\n",
      "temp ratings agg path: /mnt/bd-project/temp_ratings_agg/dt=2024-07-10\n",
      "processing item...  : dt=2024-07-10\n",
      "files successfully saved\n",
      "copy operation successful\n",
      "copy operation successful\n",
      "Operation successfully completed\n",
      "2024-07-12/\n",
      "dt=2024-07-12\n",
      "temp ratings path: /mnt/bd-project/temp-ratings-result/dt=2024-07-12\n",
      "temp ratings agg path: /mnt/bd-project/temp_ratings_agg/dt=2024-07-12\n",
      "processing item...  : dt=2024-07-12\n",
      "files successfully saved\n",
      "copy operation successful\n",
      "copy operation successful\n",
      "Operation successfully completed\n"
     ]
    }
   ],
   "source": [
    "# set trained model path and use the trained model on daily yelp data to \n",
    "# predict the ratings and to use in further data analysis\n",
    "\n",
    "\n",
    "# set trained model path and encoded label path\n",
    "ml_modelX = \"/mnt/bd-project/LRModel\"\n",
    "stringindexerY = \"/mnt/bd-project/StringIndexer\"\n",
    "\n",
    "\n",
    "kanter = 0\n",
    "# Loop through the list of folders and process each one\n",
    "for item in missing_folders:\n",
    "\n",
    "    # kanter += 1     # for testing\n",
    "    # if kanter == 3:\n",
    "    #     break\n",
    "    \n",
    "    if len(missing_folders) == 0:\n",
    "        break\n",
    "\n",
    "\n",
    "    print(item)\n",
    "    itm = \"dt=\" + item.split(\"/\")[0]\n",
    "    print(itm)\n",
    "    \n",
    "    temp_pathX = \"/mnt/bd-project/temp-ratings-result/\" \n",
    "    temp_ratings_agg_pathX = \"/mnt/bd-project/temp_ratings_agg/\" \n",
    "\n",
    "    print(\"temp ratings path: \" + temp_pathX + itm)\n",
    "    print(\"temp ratings agg path: \" + temp_ratings_agg_pathX + itm)\n",
    "\n",
    "    \n",
    "    pathX = f\"/mnt/bd-project/de-yelp-daily/{itm}/\"\n",
    "    print(f\"processing item...  : {itm}\")\n",
    "\n",
    "    postsdf = spark.read.parquet(pathX)\n",
    "    \n",
    "    # Run the UDF and generate the result\n",
    "    resultdf = transform_and_predict(postsdf, ml_modelX, stringindexerY)\n",
    "\n",
    "    tempresultdf = resultdf.drop('tokens', 'filtered', 'cv', 'features', 'rawPrediction', 'probability')\n",
    "\n",
    "    # change the column name \n",
    "    ratingsdf = tempresultdf.withColumnRenamed('decoded', 'stars')     # .select('stars')\n",
    "   \n",
    "    ratingstempdf = ratingsdf.select('stars', 'prediction')\n",
    "\n",
    "    # Aggregate the topics and calculate the total qty of each topic\n",
    "    ratings_qtydf = ratingstempdf.select('stars')\\\n",
    "        .groupBy(col(\"stars\"))\\\n",
    "            .agg(count('stars').alias('qty')).orderBy(desc('qty'))\n",
    "      \n",
    "    saveFiles_multiple_call(temp_pathX + itm, \\\n",
    "                            temp_ratings_agg_pathX + itm, \\\n",
    "                                ratingsdf, ratings_qtydf)\n",
    "    \n",
    "    # copy only aggregate ratings file from temp to final location\n",
    "    to_final_agg = \"/mnt/bd-project/bi/ml-ratings-agg/\"     \n",
    "    CopyCsv(temp_ratings_agg_pathX + itm, to_final_agg, itm)\n",
    "\n",
    "    # remove temp aggregate folder content after successful copy\n",
    "    RemoveFolderContents(temp_ratings_agg_pathX)\n",
    "        \n",
    "\n",
    "    # copy final ratings data from temp to final location\n",
    "    to_final = \"/mnt/bd-project/final-ratings/\"        \n",
    "    CopyCsv(temp_pathX + itm, to_final, itm)\n",
    "\n",
    "    # remove temp rating folder contents after successful copy\n",
    "    RemoveFolderContents(temp_pathX)\n",
    "    \n",
    "    print(\"Operation successfully completed\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922d1062-76e3-45a4-96ee-8adc2e3889ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+--------------------+--------------------+------+--------------------+-------------------+----------+-----+\n",
      "|         business_id|cool|funny|           review_id|                text|useful|             user_id|               date|prediction|stars|\n",
      "+--------------------+----+-----+--------------------+--------------------+------+--------------------+-------------------+----------+-----+\n",
      "|XMaNtA_BuemFW1fn8...|   1|    1|qUHzUc3ub6NvRkvp9...|i love this place...|     1|1eqy55tfazEXqDvGq...|2024-07-12 23:54:08|       1.0|  4.0|\n",
      "|CT2QgcDyqc5EmJ4HT...|   0|    1|O7tiIHsq41NQ4gwgZ...|im all about a ba...|     2|HTgsSLhJEcY2_V8LI...|2024-07-12 23:54:07|       0.0|  5.0|\n",
      "|ZTTMa3yZ5MmOTzbXI...|   0|    0|7KYj-G8iUnSl3KVCn...|pasta house is a ...|     1|kV7kmmBw_8IPLyI5M...|2024-07-12 23:54:08|       1.0|  4.0|\n",
      "|Xk2MNo4Kg9tikCM9Q...|   2|    0|LAo2VECR6A2DVDW3J...|full disclosure i...|     3|kV7kmmBw_8IPLyI5M...|2024-07-12 23:54:08|       0.0|  5.0|\n",
      "|n_fUROdhfmLwd_mpB...|   4|    0|U6wYm4Msk96ICP_Y7...|if you go to new ...|     4|l8tiWpQrA5-EGDAFK...|2024-07-12 23:54:07|       0.0|  5.0|\n",
      "+--------------------+----+-----+--------------------+--------------------+------+--------------------+-------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Code to check if missing_folders = 0 before displaying data in ratingsdf. \n",
    "# If missing_folders = 0, it implies no new file to process for the moment\n",
    "# This is necessary to prevent causing error in the notebook when there \n",
    "# is no new file to populate the ratingsdf \n",
    "\n",
    "if (len(missing_folders) == 0):\n",
    "    if 'ratingsdf' in globals():        # due to previous runs i.e when previous ratingsdf still in gloabls\n",
    "        del globals()['ratingsdf']      # delete the previous ratingsdf from global scope else it will display data\n",
    "        print(\"DataFrame 'ratingsdf' is not defined.\")\n",
    "    else:\n",
    "        print(\"DataFrame 'ratingsdf' is not defined.\")\n",
    "else:\n",
    "    ratingsdf.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9099e011-ddc2-45ae-8bdd-7a9f7e6ac1ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>stars</th><th>qty</th></tr></thead><tbody><tr><td>5.0</td><td>11</td></tr><tr><td>4.0</td><td>9</td></tr><tr><td>3.0</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "5.0",
         11
        ],
        [
         "4.0",
         9
        ],
        [
         "3.0",
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "stars",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "qty",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code to check if missing_folders = 0 before displaying data in ratings_qtydf. \n",
    "# If missing_folders = 0, it implies no new file to process for the moment\n",
    "# This is necessary to prevent causing error in the notebook when there no \n",
    "# new file to populate the ratings_qtydf \n",
    "\n",
    "if (len(missing_folders) == 0):\n",
    "    if 'ratings_qtydf' in globals():    # due to previous runs i.e when previous ratings_qtydf still in gloabls\n",
    "        del globals()['ratings_qtydf']  # delete the previous ratings_qtydf from global scope else it will display data\n",
    "        print(\"DataFrame 'ratings_qtydf' is not defined.\")\n",
    "    else:\n",
    "        print(\"DataFrame 'ratings_qtydf' is not defined.\")\n",
    "else:\n",
    "    display(ratings_qtydf.head(5))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4205066890262427,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Model Deployment",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
